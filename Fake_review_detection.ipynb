{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674dc37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Analyzing fake reviews per category...\n",
      "\n",
      "[INFO] Category with the most fake reviews: Kindle_Store_5 (Count: 2365)\n",
      "Top 5 Categories with Fake Reviews:\n",
      "category\n",
      "Kindle_Store_5        2365\n",
      "Books_5               2185\n",
      "Pet_Supplies_5        2127\n",
      "Home_and_Kitchen_5    2028\n",
      "Electronics_5         1994\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "Loading Spacy model...\n",
      "Error: Spacy not installed. Please run: pip install spacy\n",
      "Vectorizing text with Spacy tokenizer (Unigrams + Bigrams)...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     64\u001b[39m max_features = \u001b[32m10000\u001b[39m \n\u001b[32m     65\u001b[39m vectorizer = TfidfVectorizer(max_features=max_features, \n\u001b[32m     66\u001b[39m                              tokenizer=spacy_tokenizer, \n\u001b[32m     67\u001b[39m                              token_pattern=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     68\u001b[39m                              ngram_range=(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)) \u001b[38;5;66;03m# Use unigrams and bigrams\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m X = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext_\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.toarray()\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# 4. Label Encoding\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEncoding labels...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1386\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1378\u001b[39m             warnings.warn(\n\u001b[32m   1379\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1380\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1381\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1382\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1383\u001b[39m             )\n\u001b[32m   1384\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1389\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1273\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1272\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1273\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1274\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1275\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:115\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    113\u001b[39m     doc = preprocessor(doc)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     doc = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mspacy_tokenizer\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     56\u001b[39m text = \u001b[38;5;28mstr\u001b[39m(text).lower()\n\u001b[32m     57\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[^a-zA-Z\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text) \n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m doc = \u001b[43mnlp\u001b[49m(text)\n\u001b[32m     59\u001b[39m tokens = [token.lemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token.is_stop \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token.is_punct \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token.is_space]\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[31mNameError\u001b[39m: name 'nlp' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Load Data\n",
    "dataset_path = 'fake_reviews_dataset.csv'\n",
    "try:\n",
    "    df = pd.read_csv(dataset_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File {dataset_path} not found. Please make sure the file is in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(subset=['text_', 'label', 'category'], inplace=True)\n",
    "\n",
    "# --- Analysis: Find Category with Most Fake Reviews ---\n",
    "print(\"Analyzing fake reviews per category...\")\n",
    "fake_reviews_df = df[df['label'] == 'CG']\n",
    "category_counts = fake_reviews_df['category'].value_counts()\n",
    "if not category_counts.empty:\n",
    "    most_fake_category = category_counts.idxmax()\n",
    "    count = category_counts.max()\n",
    "    print(f\"\\n[INFO] Category with the most fake reviews: {most_fake_category} (Count: {count})\")\n",
    "    print(\"Top 5 Categories with Fake Reviews:\") \n",
    "    print(category_counts.head(5))\n",
    "else:\n",
    "    print(\"[INFO] No fake reviews found for analysis.\")\n",
    "print(\"-\" * 30)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# 2. Preprocessing & Vectorization (Spacy + TF-IDF with N-grams)\n",
    "print(\"Loading Spacy model...\")\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "except OSError:\n",
    "    print(\"Error: Spacy model 'en_core_web_sm' not found. Please run: python -m spacy download en_core_web_sm\")\n",
    "    exit()\n",
    "except ImportError:\n",
    "    print(\"Error: Spacy not installed. Please run: pip install spacy\")\n",
    "    exit()\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) \n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "    return tokens\n",
    "\n",
    "print(\"Vectorizing text with Spacy tokenizer (Unigrams + Bigrams)...\")\n",
    "# Increased max_features and added ngram_range for better accuracy\n",
    "max_features = 10000 \n",
    "vectorizer = TfidfVectorizer(max_features=max_features, \n",
    "                             tokenizer=spacy_tokenizer, \n",
    "                             token_pattern=None,\n",
    "                             ngram_range=(1, 2)) # Use unigrams and bigrams\n",
    "\n",
    "X = vectorizer.fit_transform(df['text_']).toarray()\n",
    "\n",
    "# 4. Label Encoding\n",
    "print(\"Encoding labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['label'])\n",
    "print(f\"Label Mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "# 5. Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. PyTorch Dataset\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.FloatTensor(labels) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = ReviewDataset(X_train, y_train)\n",
    "test_dataset = ReviewDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 7. Neural Network Architecture (Improved)\n",
    "class BinaryNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BinaryNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256) # Increased neurons\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4) # Increased dropout\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "input_dim = max_features\n",
    "model = BinaryNN(input_dim).to(device)\n",
    "\n",
    "# 8. Training Loop\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005) # Lower learning rate for better convergence\n",
    "\n",
    "num_epochs = 8 # Increased epochs\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        labels = labels.unsqueeze(1) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 9. Evaluation\n",
    "print(\"Evaluating model...\")\n",
    "model.eval()\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        predicted = torch.round(torch.sigmoid(outputs)) \n",
    "        \n",
    "        # Robust scalar conversion\n",
    "        y_pred_list.extend(predicted.squeeze().detach().cpu().tolist())\n",
    "        y_true_list.extend(labels.squeeze().detach().cpu().tolist())\n",
    "\n",
    "y_pred_list = [int(i) for i in y_pred_list] \n",
    "y_true_list = [int(i) for i in y_true_list]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_list, y_pred_list, target_names=label_encoder.classes_))\n",
    "print(f\"Accuracy: {accuracy_score(y_true_list, y_pred_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10670fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
